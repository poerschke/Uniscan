# language file
lang=en

#  max_threads is the integer number of maximum threads uniscan will use
max_threads=15

# max_reqs is the integer number of maximum requestes that crawler can do
max_reqs=15000

# timeout is the time in seconds that uniscan wait for a connection to webserver
timeout=10

use_proxy=0

# for use proxy you need change de value of proxy from 0.0.0.0 to your proxy ip
proxy=0.0.0.0

# for use proxy you need change de value of proxy_port from 65000 to your proxy port
proxy_port=65000

# log file of uniscan
log_file=uniscan.log

# Current version of uniscan
version=6.3

# maximum size of one request in bytes
max_size=1048576

# maximum variation of a page
variation=15

# extensions that uniscan will ignore on crawler
extensions=.wmv.exe.pdf.xls.csv.mdb.rpm.deb.doc.odt.pptx.docx.db.xps.cdr.jpg.jpeg.png.gif.bmp.css.tgz.gz.bz2.zip.rar.tar.asf.avi.bin.dll.js.fla.mp3.mpg.mov.ogg.ppt.rtf.scr.wav.msi.swf.sql.xml

# codes acceptable by uniscan in directory, files and backup file checks
code=200|302|304

# the return of file http://uniscan.sourceforge.net/c.txt used in RFI checks
rfi_return=unipampascanunipampa

# 0 = disable basic auth, 1 = enable
use_basic_auth=0

# username to basic auth
basic_login=admin

# password to basic auth
basic_pass=admin

# 0 = disable cookie auth, 1 = enable
use_cookie_auth=0

# here is the url to post the login form content
url_cookie_auth=http://192.168.1.102/login.php

# method = POST or GET
method_cookie_login=POST

# here is the where you put the inputs and the values of login form
input_cookie_login="&login=admin&senha=adm"

# to use urlencode set url_encode = 1 and 0 to disable
# the | and % will not be encoded because RCE does not work if it is encoded
url_encode=0

# the user-agent of uniscan
user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.77 Safari/535.7"

# this config enable/disable redirect detection
redirect=1

#report file
html_report=report/uniscan.html

# enable/disable auto-update using git
autoupdate=1

# show ignored files by crawler
show_ignored=1

# show ignored images by crawler
show_images=0

# write requests on requests.txt
write_reqs=0

# force backup and fckeditor checks
force_bf=0
